
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}

#+latex_engraved_theme: t

#+PROPERTY: header-args :eval always-export
#+OPTIONS: toc:nil
#+OPTIONS: num:nil
#+TITLE: Homework 2
#+AUTHOR: Malcolm, Vejay, Rohan, Tyler
#+DATE: \today
#+SUBTITLE: STA 307

#+attr_latex: :engraved-theme doom-gruvbox-light
#+begin_src python :session graphics :results output :exports both :tangle project1.py
# Imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from ISLP import load_data
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
AUTO = load_data("Auto")
#+end_src

#+RESULTS:

* Question 1


#+begin_src python :session graphics :results output :exports both :tangle project1.py
# Question1
selected_coloumns = ['mpg','cylinders', 'displacement',  'weight', 'acceleration', ]
X = AUTO[selected_coloumns]
y = AUTO['origin'] # Origin of car (1. American, 2. European, 3. Japanese)
print(X.head)
print("American cars: {0}".format((y == 1).sum())) # American
print("European cars: {0}".format((y == 2).sum())) # European
print("Japanese cars: {0}".format((y == 3).sum())) # Japanese
#+end_src

#+RESULTS:
#+begin_example
American cars: 245
European cars: 68
Japanese cars: 79
<bound method NDFrame.head of       mpg  cylinders  displacement  weight  acceleration
0    18.0          8         307.0    3504          12.0
1    15.0          8         350.0    3693          11.5
2    18.0          8         318.0    3436          11.0
3    16.0          8         304.0    3433          12.0
4    17.0          8         302.0    3449          10.5
..    ...        ...           ...     ...           ...
387  27.0          4         140.0    2790          15.6
388  44.0          4          97.0    2130          24.6
389  32.0          4         135.0    2295          11.6
390  28.0          4         120.0    2625          18.6
391  31.0          4         119.0    2720          19.4

[392 rows x 5 columns]>
#+end_example

* Question 2



#+begin_src python :session graphics :results output :exports both :tangle project1.py
# Question2
print("Standard deviation")
print(X.std())
print("\n")
print("Mean")
print(X.mean())
#+end_src

#+RESULTS:
#+begin_example
Standard deviation
mpg               7.805007
cylinders         1.705783
displacement    104.644004
weight          849.402560
acceleration      2.758864
dtype: float64


Mean
mpg               23.445918
cylinders          5.471939
displacement     194.411990
weight          2977.584184
acceleration      15.541327
dtype: float64
#+end_example

* Qustion 3

** a
#+begin_src python :session graphics :results graphics file value :file plot.png :exports both :eval yes :tangle project1.py
# Question3_parta
print("Original data size:",X.shape)
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)
print("Reduced data size:", X_reduced.shape)
components = pca.components_
x = np.arange(components.shape[1]) # 6
plt.plot(x, components[0], label='φ1')
plt.plot(x, components[1], label='φ2')
names = ['mpg', 'cylinders', 'displacement', 'weight', 'acceleration']
plt.xticks(ticks=x, labels=selected_coloumns, rotation=45)
plt.xlabel('Features')
plt.ylabel('Principal Component Weights')
plt.title('Principal Component Weights per Feature')
plt.legend()
plt.tight_layout()
plt.savefig("plot.png", bbox_inches='tight')
plt.close()
#+end_src

#+RESULTS:
[[file:plot.png]]

** b
It seems pca is caputring the data with larger scales this can lead to misleading conclusions about the importance of features

* Question 4


#+begin_src python :session graphics :results output :exports both :eval yes :tangle project1.py
# Question4
means = np.mean(X, axis=0)
stds = np.std(X, axis=0)
Z = (X - means) / stds
means_Z = np.mean(Z, axis=0)
stds_Z = np.std(Z, axis=0)
print(means_Z, stds_Z)
#+end_src

#+RESULTS:
#+begin_example
mpg             1.450087e-16
cylinders      -1.087565e-16
displacement   -7.250436e-17
weight         -1.812609e-17
acceleration    4.350262e-16
dtype: float64 mpg             1.0
cylinders       1.0
displacement    1.0
weight          1.0
acceleration    1.0
dtype: float64
#+end_example

* Question 5

** a
#+begin_src python :session graphics :results graphics file value :file plot-standard.png :exports both :eval yes :tangle project1.py
# Question5_parta
pca = PCA(n_components=2)
Z_reduced = pca.fit_transform(Z)
print("Reduced data size:", Z_reduced.shape)
components = pca.components_
plt.plot(x, components[0], label='φ1')
plt.plot(x, components[1], label='φ2')
plt.xticks(ticks=x, labels=selected_coloumns, rotation=45)
plt.xlabel('Features')
plt.ylabel('Principal Component Weights')
plt.title('Principal Component Weights per Feature')
plt.legend()
plt.tight_layout()
plt.savefig("plot-standard.png", bbox_inches='tight')
plt.close()
# plt.show()
#+end_src

#+RESULTS:
[[file:plot-standard.png]]

** b
It seems that the three most important features are 'cylinders', 'displacement', and 'weight', as the first principal component (\(\phi\)1) weighs these more heavily, with 'displacement' appearing to have the highest contribution to variance by a small margin.
** c

#+begin_src python :session graphics :results output :exports both :eval yes :tangle project1.py
# Question5_partc
dot_product_mock = np.dot(components[0], components[1])
# Calculate the magnitude (norm) of each principal component to check if it's equal to one
magnitude_phi1_mock = np.linalg.norm(components[0])
magnitude_phi2_mock = np.linalg.norm(components[1])
print("Dot product", dot_product_mock)
print("Magnitude phi 1",magnitude_phi1)
print("Magnitude phi 2", magnitude_phi2)

#+end_src

#+RESULTS:
: Dot product -1.1102230246251565e-16
: Magnitude phi 1 1.0000000000000002
: Magnitude phi 2 1.0000000000000002

* Question 6
** a
#+begin_src python :session graphics :results graphics file value :file plot-scatter.png :exports both  :tangle project1.py
# Question6_parta
origins = ["American", "European", "Japanese"]
# plt.figure(figsize=(8, 6))
for origin in [1, 2, 3]:
    subset = Z_reduced[origin == y]
    plt.scatter(subset[:, 0], subset[:, 1], label=origins[origin - 1], alpha=0.7)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of Car Dataset by Origin')
plt.legend(title='Car Origin')
plt.tight_layout()
plt.savefig("plot-scatter.png")
plt.close()
#+end_src

#+RESULTS:
[[file:plot-scatter.png]]

** b


#+begin_src python :session graphics :results output :exports both :eval yes :tangle project1.py
# Assuming AUTO is a pandas DataFrame containing the dataset

# Calculate mean value of the feature (e.g., 'mpg') for each class
mean_mpg_american = AUTO[AUTO['origin'] == 1]['mpg'].mean()
mean_mpg_european = AUTO[AUTO['origin'] == 2]['mpg'].mean()
mean_mpg_japanese = AUTO[AUTO['origin'] == 3]['mpg'].mean()

# Display the mean values
print("Mean MPG for American cars:", mean_mpg_american)
print("Mean MPG for European cars:", mean_mpg_european)
print("Mean MPG for Japanese cars:", mean_mpg_japanese)

# Determine which class has the largest mean value for the feature
if mean_mpg_american > mean_mpg_european and mean_mpg_american > mean_mpg_japanese:
    print("American cars have the largest mean MPG.")
elif mean_mpg_european > mean_mpg_american and mean_mpg_european > mean_mpg_japanese:
    print("European cars have the largest mean MPG.")
else:
    print("Japanese cars have the largest mean MPG.")

#+end_src

#+RESULTS:
: Mean MPG for American cars: 20.0334693877551
: Mean MPG for European cars: 27.602941176470587
: Mean MPG for Japanese cars: 30.450632911392404
: Japanese cars have the largest mean MPG.

** c
* Question 7
** a
** b
** c
